---
title: "Inferencia variacional y ELBO Loss"
description: "Derivaci√≥n paso a paso de la funci√≥n de p√©rdida ELBO en VAEs"
author: "Alex de la Puente"
date: "2026-01-25"
categories: [machine-learning, vae, matematicas]
format: html
execute:
  enabled: true
jupyter: python3
---



Bueno, pues vamos a intentar derivar la funcion de p√©rdida de un autoencoder variacional.

En un autoencoder normal, lo que hacemos es usar el error de reconstrucci√≥n como funci√≥n 
de p√©rdida. Pero, y ¬øLuego que? ¬øC√≥mo lo hacemos variacional? ¬øY que ~~co√±o~~ es variacional? 
¬øQue son todas esas historias del ELBO, el KL Divergence y dem√°s jerogl√≠ficos arcanos?



Paso a paso. Vamos a ver: 

## Del autoencoder convencional al variacional

Aqu√≠ tenemos el esquema de un vanila autoencoder.

::: {#fig-vae1 style="float: right; margin-right: 20px; margin-bottom: 20px;"}
```{.tikz}
%%| filename: vae1
%%| format: svg

\usetikzlibrary{positioning,arrows.meta}

\begin{tikzpicture}[
  node distance=1.8cm,
  >=Latex,
  scale=1.2,
  transform shape,
  every node/.style={font=\large},
  arrow/.style={->, thick}
]

% Variables con texto debajo
\node (x) {$x$};
\node[below=0.4cm of x, font=\small] {Input};

% Encoder (trapecio: ancho a la izquierda, estrecho a la derecha)
\node[right=1.8cm of x] (enc_center) {};
\draw[thick] 
  ([xshift=-1cm, yshift=0.6cm] enc_center.center) -- 
  ([xshift=1cm, yshift=0.4cm] enc_center.center) -- 
  ([xshift=1cm, yshift=-0.4cm] enc_center.center) -- 
  ([xshift=-1cm, yshift=-0.6cm] enc_center.center) -- cycle;
\node at (enc_center) {\textbf{Encoder}};

% Variable latente
\node[right=1.8cm of enc_center] (z) {$z$};
\node[below=0.4cm of z, font=\small] {Latent};

% Decoder (trapecio: estrecho a la izquierda, ancho a la derecha)
\node[right=1.8cm of z] (dec_center) {};
\draw[thick] 
  ([xshift=-1cm, yshift=0.4cm] dec_center.center) -- 
  ([xshift=1cm, yshift=0.6cm] dec_center.center) -- 
  ([xshift=1cm, yshift=-0.6cm] dec_center.center) -- 
  ([xshift=-1cm, yshift=-0.4cm] dec_center.center) -- cycle;
\node at (dec_center) {\textbf{Decoder}};

% Reconstrucci√≥n
\node[right=1.8cm of dec_center] (xhat) {$\hat{x}$};
\node[below=0.4cm of xhat, font=\small] {Reconstruction};

% Flechas
\draw[arrow] (x) -- ([xshift=-1cm] enc_center.center);
\draw[arrow] ([xshift=1cm] enc_center.center) -- (z);
\draw[arrow] (z) -- ([xshift=-1cm] dec_center.center);
\draw[arrow] ([xshift=1cm] dec_center.center) -- (xhat);

\end{tikzpicture}
```

VAE diagram
:::

Los datos de entrada $x$ se comprimen por el encoder en un espacio latente $z$ que luego 
el decoder toma como input para reconstruir $\hat{x}$ usando como funci√≥n de p√©rdida el 
error de reconstrucci√≥n (algo como el MSE, o el MAE, lo que queramos). 

Hasta aqui todo bien, todo correcto. ¬øCu√°l es el problema? Pues que sabe di√≥s la forma que 
tomar√° ese espacio latente $z$, porque no tenemos ning√∫n tipo de control sobre ella.

Dado que ese espacio latente puede tomar formas incomodas con las que prefeririamos no tener
que tratar, lo que vamos a hacer es mapearlo a una distribuci√≥n gaussiana. Esto es lo que convierte a nuestro
autoencoder cl√°sico en uno **variacional**:

$$P(z|x) \sim N(0,1)$$

Ahora, lo que queremos es que la probabilidad de $z$ dado $x$ se parezca lo m√°ximo posible a una 
distribuci√≥n gaussiana est√°ndar (la de media 0 y desviaci√≥n 1, vaya).

::: {.callout-note collapse="true"}
## T√≠tulo de la nota (clic para expandir)

Aqu√≠ va el contenido oculto que se despliega al hacer clic.
Puedes incluir ecuaciones, c√≥digo, im√°genes, etc.
:::

¬øY como se hace esto? Pues a√±adiendo un segundo t√©rmino a nuestra funcion de p√©rdida basada 
en la reconstrucci√≥n que ten√≠amos antes. 

Lo que queremos es una nueva funci√≥n de p√©rdida que penalize 
al modelo si no es capaz de que $z$ se aproxime a una gaussiana. Hay varias manera de 
hacerlo (por lo visto), pero la mas habitual es la KL divergence. 

De manera que nuestra nueva y awesome loss function nos quedar√≠a tal que as√≠:


$$
\begin{aligned}
\mathcal{L} &= \mathcal{L}_{\mathrm{reconstruction}} + D_{\mathrm{KL}}
\end{aligned}
$$

## KL Divergence

Una divergencia es una manera de medir la distancia entre distribuciones. Es decir, 
estas dos distribuciones ¬øcu√°nto se parecen? ¬øMucho?¬øPoco? ¬øEst√°n cerca? ¬øEst√°n lejos? Evidentemente 
si estan cerca es que *se parecen* mucho, y sino, es que se parecen poco. 

Matematicamente, mediamos la distancia entre dos distribuciones $P$ y $Q$ como:

$$
D_{KL}(P \| Q) = \sum_{x \in \mathrm{X}} p(x) \log \frac{p(x)}{q(x)} \quad \text{o} \quad \int p(x) \log \frac{p(x)}{q(x)} \, dx
$$

La primera en su forma discreta y la segunda en su forma continua, pero son equivalentes. Algunas veces 
lo podremos ver expresado en funci√≥n de la esperanza tambi√©n. Pero vamos que es lo mismo. 

$$
D_{KL}(P \| Q) = \mathbb{E}_{p} \left[ \log \frac{p(x)}{q(x)} \right]
$$

Tambi√©n podemos reorganizar los t√©rminos de la expresi√≥n discreta para obtener:

$$
D_{KL}(P \| Q) = \underbrace{\sum_{x \in \mathrm{X}} p(x) \log p(x)}_{-H(P) \, \text{Negative Entropy}} - \underbrace{\sum_{x \in \mathrm{X}} p(x) \log q(x)}_{CE(P,Q) \, \text{Negative Cross Entropy}}
$$

Esta reorganizaci√≥n tampoco es que nos diga mucho ahora mismo, pero podriamos reescribir la divergencia:

$$
D_{KL} = CE(P,Q) - H(P) \Leftrightarrow CE(P,Q) = D_{KL} + H(P)
$$

Esta identidad tiene su origen en la teor√≠a de la informaci√≥n de Shannon, pero ni es el tema de hoy, ni nos ayuda 
a deribar y entener la inferencia variacional ni nada, por lo que lo dejamos en el caj√≥n para otro d√≠a. De momento,
solo nos tenemos que quedar con esto: 

$$
D_{KL}(P \| Q) = \sum_{x \in \mathrm{X}} p(x) \log \frac{p(x)}{q(x)}
$$

Otra cosa que tambi√©n vamos a necesitar es el comprender por qu√© $D_{KL} \geq 0$, tambi√©n llamada **Desigualdad de Gibbs**.



```{python}
#| echo: false
#| output: false
import matplotlib
matplotlib.use('Agg')
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams.update({
    'font.size': 18,
    'axes.titlesize': 16,
    'axes.labelsize': 16,
    'xtick.labelsize': 14,
    'ytick.labelsize': 14,
    'legend.fontsize': 16,
    'figure.titlesize': 20
})

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# === C√ìNCAVA: log(x) ===
x = np.linspace(0.1, 5, 200)
y = np.log(x)

ax1.plot(x, y, 'b-', linewidth=2.5, label=r'$\log(x)$')

x1, x2 = 0.5, 4
y1, y2 = np.log(x1), np.log(x2)
ax1.plot([x1, x2], [y1, y2], 'r--', linewidth=2)
ax1.scatter([x1, x2], [y1, y2], color='red', s=80, zorder=5)

x_mid = (x1 + x2) / 2
y_curva = np.log(x_mid)
y_linea = (y1 + y2) / 2

ax1.scatter([x_mid], [y_curva], color='green', s=100, zorder=5, label=r'$f(\mathbb{E}[X])$')
ax1.scatter([x_mid], [y_linea], color='orange', s=100, zorder=5, label=r'$\mathbb{E}[f(X)]$')

# Etiquetas con notaci√≥n lambda
ax1.annotate(r'$(x_1, f(x_1))$', (x1, y1), textcoords="offset points", xytext=(10, -10))
ax1.annotate(r'$(x_2, f(x_2))$', (x2, y2), textcoords="offset points", xytext=(5, -15))
ax1.annotate(r'$\lambda_1 f(x_1) + \lambda_2 f(x_2)$', (x_mid, y_linea), textcoords="offset points", xytext=(15, -12), color='orange')
ax1.annotate(r'$f(\lambda_1 x_1 + \lambda_2 x_2)$', (x_mid, y_curva), textcoords="offset points", xytext=(-110, 12),  color='green')

ax1.axhline(y=0, color='k', linewidth=0.5)
ax1.axvline(x=0, color='k', linewidth=0.5)
ax1.set_xlabel('x')
ax1.set_ylabel(r'$f(x)$')
ax1.set_title(r'C√≥ncava: $f(x) = \log(x)$ ‚Üí $\mathbb{E}[f(X)] \leq f(\mathbb{E}[X])$')
ax1.legend(loc='lower right')
ax1.grid(True, alpha=0.3)

# === CONVEXA: x¬≤ ===
x = np.linspace(-2, 3, 200)
y = x**2

ax2.plot(x, y, 'b-', linewidth=2.5, label=r'$x^2$')

x1, x2 = -1, 2.5
y1, y2 = x1**2, x2**2
ax2.plot([x1, x2], [y1, y2], 'r--', linewidth=2)
ax2.scatter([x1, x2], [y1, y2], color='red', s=80, zorder=5)

x_mid = (x1 + x2) / 2
y_curva = x_mid**2
y_linea = (y1 + y2) / 2

ax2.scatter([x_mid], [y_curva], color='green', s=100, zorder=5, label=r'$f(\mathbb{E}[X])$')
ax2.scatter([x_mid], [y_linea], color='orange', s=100, zorder=5, label=r'$\mathbb{E}[f(X)]$')

# Etiquetas con notaci√≥n lambda
ax2.annotate(r'$(x_1, f(x_1))$', (x1, y1), textcoords="offset points", xytext=(-85, -5))
ax2.annotate(r'$(x_2, f(x_2))$', (x2, y2), textcoords="offset points", xytext=(-85, 0))
ax2.annotate(r'$\lambda_1 f(x_1) + \lambda_2 f(x_2)$', (x_mid, y_linea), textcoords="offset points", xytext=(-145, 2),  color='orange')
ax2.annotate(r'$f(\lambda_1 x_1 + \lambda_2 x_2)$', (x_mid, y_curva), textcoords="offset points", xytext=(15, -2),  color='green')

ax2.axhline(y=0, color='k', linewidth=0.5)
ax2.axvline(x=0, color='k', linewidth=0.5)
ax2.set_xlabel('x')
ax2.set_ylabel(r'$f(x)$')
ax2.set_title(r'Convexa: $f(x) = x^2$ ‚Üí $\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])$')
ax2.legend(loc='upper left', framealpha=1)
ax2.grid(True, alpha=0.3)

plt.tight_layout(w_pad=3)
plt.savefig('jensen_plot.svg', format='svg', bbox_inches='tight')
plt.close()
```

::: {.callout-note collapse="true"}
## Prueba: $D_{KL} \geq 0$ (Desigualdad de Gibbs)

La **desigualdad de Gibbs** establece que la divergencia KL es siempre no negativa:

$$
D_{KL}(P \| Q) \geq 0
$$

Menos cuando $P=Q$, que ser√° cuando las dos distribuciones sean id√©nticas, y por tanto, 
la distancia entre ellas sea 0. Este resultado es 
fundamental en teor√≠a de la informaci√≥n y lleva el nombre del f√≠sico Josiah Willard Gibbs. 
Para demostrarla, utilizamos la **desigualdad de Jensen**.

### Recordatorio: Desigualdad de Jensen

La desigualdad de Jensen relaciona el valor esperado de una funci√≥n con la funci√≥n del valor esperado:

- **Funci√≥n c√≥ncava $\rightarrow$ **  $\mathbb{E}[f(X)] \leq f(\mathbb{E}[X])$: Para una funci√≥n 
concava, el valor esperado de su funci√≥n siempre ser√° **menor** que la funci√≥n de su valor esperado (ej: $\log$). 
- **Funci√≥n convexa $\rightarrow$ ** $\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])$: Para una funci√≥n 
convexa, el valor esperado de la funci√≥n siempre sera **mayor** que la funci√≥n de su valor esperado (ej: $x^2$).

![Desigualdad de Jensen](jensen_plot.svg){width=100%}


En los gr√°ficos tenemos dos valores de ejemplo: $x_1$ y $x_2$ (puntos rojos sobre la curva).

- **L√≠nea roja discontinua**: conecta los dos puntos rojos. Representa qu√© pasar√≠a si interpol√°ramos 
linealmente entre $f(x_1)$ y $f(x_2)$.

- **Punto naranja** $\mathbb{E}[f(X)]$: es el promedio de $f(x_1)$ y $f(x_2)$. Como promediar es una 
operaci√≥n lineal, este punto cae sobre la l√≠nea roja.

- **Punto verde** $f(\mathbb{E}[X])$: primero calculamos el promedio de $x_1$ y $x_2$, y luego 
evaluamos $f$ en ese punto. Por eso cae directamente sobre la curva.


Para funciones **c√≥ncavas**, la curva queda por encima de la l√≠nea roja que conecta los puntos, as√≠ 
que el punto verde siempre estar√° m√°s arriba que el naranja ($f(\mathbb{E}[X]) \geq \mathbb{E}[f(X)]$). Para 
funciones **convexas**, la curva queda por debajo, as√≠ que el punto naranja siempre estar√° 
m√°s arriba que el verde ($\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])$).


### Demostraci√≥n de Gibbs usando Jensen {#gibbs}

Una vez hemos entendido esto, volvemos a nuestra definici√≥n del KL Divergence:

$$
D_{KL}(P \| Q) = \sum_x p(x) \log \frac{p(x)}{q(x)}
$$

La cual podemos reescribir dando la vuelta a los t√©rminos:

$$
 D_{KL}(P \| Q) = -\sum_x p(x) \log \frac{q(x)}{p(x)}
$$

Como $\mathbb{E}[X] = \sum_{x} x \cdot p(x)$, podemos reescribir la expresi√≥n en t√©rminos de esperanza:

$$
 D_{KL}(P \| Q) =  -\mathbb{E}_p\left[\log \frac{q(x)}{p(x)}\right]
$$

Y como la funci√≥n del logaritmo es **c√≥ncava**, podemos aplicar Jensen y decir que el valor esperado 
de nuestra funci√≥n ser√° menor que la funci√≥n del valor esperado:

$$
 \mathbb{E}_p\left[\log \frac{q(x)}{p(x)}\right] \leq \log \left( \mathbb{E}_p\left[\frac{q(x)}{p(x)}\right] \right)
$$


Evaluamos la esperanza del lado derecho:

$$
\mathbb{E}_p\left[\frac{q(x)}{p(x)}\right] = \sum_x \cancel{p(x)} \frac{q(x)}{\cancel{p(x)}} = \sum_x q(x)
$$

Y como el sumatorio de una distribuci√≥n es 1, nos queda:

$$
\mathbb{E}_p\left[\frac{q(x)}{p(x)}\right] = \sum_x q(x) = 1
$$


Volvemos a la desigualdad de Jensen con este resultado:

$$
 \mathbb{E}_p\left[\log \frac{q(x)}{p(x)}\right] \leq \log(1) = 0
$$

Ahora lo sustituimos en la expresi√≥n que ten√≠amos antes, la del KL Divergence:
$$
  D_{KL}(P \| Q) = -\mathbb{E}_p\left[\log \frac{q(x)}{p(x)}\right] \geq 0
$$

Que es lo mismo que:

$$
  D_{KL}(P \| Q) = \mathbb{E}_p\left[\log \frac{p(x)}{q(x)}\right] \geq  0
$$

Lo cual podemos reescribir de la misma manera que ten√≠amos al principio:

$$
  D_{KL}(P \| Q) = \sum_x p(x) \log \frac{p(x)}{q(x)} \geq  0
$$


¬°Tach√°n! Aqu√≠ es donde quer√≠amos llegar. Ya sabemos que la KL divergence siempre tiene que ser mayor que 0:
$$
{D_{KL}(P \| Q) \geq 0}
$$

Esto completa la demostraci√≥n de la **desigualdad de Gibbs** utilizando la **desigualdad de Jensen**.
:::


## Variational Inference

Ya hemos aprendido unas cuantas cosas. Ahora nos toca hablar sobre la inferencia variacional. 

Un autoencoder variacional no es m√°s que una forma de hacer inferencia variacional, la cual se basa en la regla de Bayes:


::: {#fig-bayes}
```{.tikz}
%%| filename: bayes_rule
%%| format: svg

\usetikzlibrary{positioning,arrows.meta}

\begin{tikzpicture}[
  >=Latex,
  scale=1.3,
  transform shape,
  every node/.style={font=\large},
  label/.style={font=\small\bfseries},
  arrow/.style={->, thick}
]

% Fraction line first (as reference)
\node (frac_center) {};
\draw[thick] ([xshift=-1.4cm]frac_center.center) -- ([xshift=1.4cm]frac_center.center);

% Numerator (closer to line)
\node[above=0.05cm of frac_center] (numerator) {$P(B|A) \cdot P(A)$};

% Denominator (closer to line)
\node[below=0.05cm of frac_center] (evidence) {$P(B)$};

% Equals and posterior aligned with fraction line
\node[left=1.6cm of frac_center] (equals) {$=$};
\node[left=0.1cm of equals] (posterior) {$P(A|B)$};

% Labels with angled arrows and colors
\node[below left=0.6cm and 0.3cm of posterior, label, color=blue!70!blue] (post_label) {Posterior};
\draw[arrow, blue!70!black] (post_label) -- (posterior.south west);

\node[above left=0.5cm and 0.8cm of numerator, label, color=red!70!black] (lik_label) {Likelihood};
\draw[arrow, red!70!black] (lik_label) -- ([xshift=-0.6cm]numerator.north);

\node[above right=0.5cm and 0.6cm of numerator, label, color=green!50!black] (prior_label) {Prior};
\draw[arrow, green!50!black] (prior_label) -- ([xshift=0.6cm]numerator.north);

\node[below right=0.5cm and 0.3cm of evidence, label, color=orange!80!black] (ev_label) {Evidence/Marginal};
\draw[arrow, orange!80!black] (ev_label) -- (evidence.south east);

\end{tikzpicture}

```
Bayes' rule
:::


::: {.callout-note collapse="true"}
## Repaso Regla de Bayes

Conviene repasar brevemente de donde sale el chisme este. 

### Probabilidad conjunta y condicional

Vamos a suponer que tenemos unos datos que observamos $x$ y unas variables latentes $z$ 
no observamos directamente. La probabilidad de que ocurran ambos a la vez es la **probabilidad conjunta $p(x,z)$**.

Esta probabilidad conjunta siempre se puede factorizar usando la definici√≥n de **probabilidad condicional**^[Probabilidad de que ocurra algo dado que otra cosa ya haya ocurrido]:

$$p(x,z) = p(z|x) \cdot p(x)$$

Que tambi√©n podemos escribir asi:

$$p(x,z) = p(x|z) \cdot p(z)$$

Igualando ambas expresiones y despejando $p(z|x)$ ¬°¬°nos sale Bayes!!:

$$p(z|x) \cdot p(x) =  p(x|z) \cdot p(z) $$
$$p(z|x)  =  \frac{p(x|z) \cdot p(z)}{p(x)} $$

As√≠ es como obtenemos los t√©rminos:

* **Posterior $p(z|x)$:** distribuci√≥n de $z$ despu√©s de haber observado $x$. Esto es lo 
que queremos calcular, y muchas veces lo veremos referido como la "actualizacion de nuestras 
creencias previas  (prior) tras haber observado los datos (likelihood)".
* **Prior $p(z)$:** Lo que creiamos sobre $z$ antes de observar los datos $x$. Esto 
quiz√°s es lo m√°s tricky. Una manera f√°cil de tener cierta intuici√≥n sobre esto es que 
si tuviesemos un dado de 6 caras y $z$ representa nuestra hip√≥tesis de si esta cargado o no ($z=\{justo, cargado \}$);
si nunca lo he lanzado pero conf√≠o en que es justo, mi prior ser√≠a algo como 
$p(z=\text{justo})=0.9$ y $p(z=\text{cargado})=0.1$. 
* **Likelihood $p(x|z)$:** probabilidad^[en puridad matem√°tica, el likelihood es **una funcion de $z$**, no 
una distribuci√≥n de probabilidad (no tiene por qu√© sumar 1 sobre $z$). Pero para este contexto did√°ctico-educativo 
es v√°lido llamarlo *probabilidad*] de observar los datos $x$ dado $z$. Responde a la pregunta: si este 
valor de $z$ fuese correcto, ¬øc√≥mo de probable ser√≠a ver estos datos $x$?. En el ejemplo del dado: si mi 
hip√≥tesis es que el dado es justo ($z=\text{justo}$), la probabilidad de sacar un 3 es $p(x=3|z=\text{justo})=1/6$. 
Si mi hip√≥tesis es que est√° cargado para sacar 6, entonces $p(x=3|z=\text{cargado})$ ser√≠a mucho menor.
* **Evidence o Marginal $p(x)$:** Probabilidad total de observar $x$, considerando todos los posibles valores de $z$. Se obtiene
marginalizando la conjunta. 

### El problema con la marginal

La evidence $p(x)$ tambi√©n se conoce como ***el marginal*** porque se obtiene 
**marginalizando**^[Marginalizar = integrar (o sumar, en discreto) sobre todas las posibilidades 
de una variable para *eliminarla*] la conjunta sobre $z$.

Si partimos de la probabilidad conjunta $p(x,z)$, marginalizar consiste en integrar sobre $z$ para 
quedarnos solo con la probabilidad de $x$ (tambi√©n podr√≠a ser al rev√©s, claro est√°):

$$p(x,z) \rightarrow p(x) = \int{p(x|z)p(z)dz}$$

El problema es que esa integral es **intratable**^[No tiene soluci√≥n anal√≠tica o es computacionalmente inviable de calcular],
porque requerir√≠a integrar sobre todas las configuraciones posibles de $z$, las cuales desconocemos. Esto es 
por lo que no podemos calcular $p(x)$ directamente en la mayor√≠a de casos. Porque:

* El espacio latente $z$ puede ser de alta dimensi√≥n.
* No tiene soluci√≥n anal√≠tica cerrada.
* Crece exponencialmente con la complejidad del modelo


:::

O sea, lo que hace Bayes b√°sicamente es obtener la posterior. ¬øNo es eso lo que hace nuestro encoder? Tenemos unos datos $x$
y queremos encontrar una representaci√≥n latente $z$ que los explique. Es decir, queremos $p(z|x)$


Sabiendo esto, podemos actualizar nuestro esquema del autoencoder en t√©rminos de Bayes y decir que:

* El **encoder aproxima la posterior**: calcula $q_{\phi}(z|x)$, la distribuci√≥n sobre los latentes $z$ mediante los 
par√°metros del modelo ${\phi}$ dados los datos observados $x$. 
* El **decoder modela el likelihood**: calcula $p_{\theta}(x|z)$, la distribuci√≥n sobre los datos reconstruidos 
$\hat{x}$ mediante los par√°metros del modelo ${\theta}$ dado el latente $z$.

::: {#fig-vae2}
```{.tikz}
%%| filename: vae2
%%| format: svg

\usetikzlibrary{positioning,arrows.meta}

\begin{tikzpicture}[
  node distance=1.8cm,
  >=Latex,
  scale=1.2,
  transform shape,
  every node/.style={font=\large},
  arrow/.style={->, thick}
]

% Variables con texto debajo
\node (x) {$x$};
\node[below=0.4cm of x, font=\small] {Input};

% Encoder (trapecio: ancho a la izquierda, estrecho a la derecha)
\node[right=1.8cm of x] (enc_center) {};
\draw[thick] 
  ([xshift=-1cm, yshift=0.7cm] enc_center.center) -- 
  ([xshift=1cm, yshift=0.5cm] enc_center.center) -- 
  ([xshift=1cm, yshift=-0.5cm] enc_center.center) -- 
  ([xshift=-1cm, yshift=-0.7cm] enc_center.center) -- cycle;
\node[yshift=0.15cm] at (enc_center) {\textbf{Encoder}};
\node[yshift=-0.25cm, font=\small] at (enc_center) {$q_\phi(z|x)$};

% Variable latente
\node[right=1.8cm of enc_center] (z) {$z$};
\node[below=0.4cm of z, font=\small] {Latent};

% Decoder (trapecio: estrecho a la izquierda, ancho a la derecha)
\node[right=1.8cm of z] (dec_center) {};
\draw[thick] 
  ([xshift=-1cm, yshift=0.5cm] dec_center.center) -- 
  ([xshift=1cm, yshift=0.7cm] dec_center.center) -- 
  ([xshift=1cm, yshift=-0.7cm] dec_center.center) -- 
  ([xshift=-1cm, yshift=-0.5cm] dec_center.center) -- cycle;
\node[yshift=0.15cm] at (dec_center) {\textbf{Decoder}};
\node[yshift=-0.25cm, font=\small] at (dec_center) {$p_\theta(x|z)$};

% Reconstrucci√≥n
\node[right=1.8cm of dec_center] (xhat) {$\hat{x}$};
\node[below=0.4cm of xhat, font=\small] {Reconstruction};

% Flechas
\draw[arrow] (x) -- ([xshift=-1cm] enc_center.center);
\draw[arrow] ([xshift=1cm] enc_center.center) -- (z);
\draw[arrow] (z) -- ([xshift=-1cm] dec_center.center);
\draw[arrow] ([xshift=1cm] dec_center.center) -- (xhat);

\end{tikzpicture}
```
VAE diagram
:::

Esto es interesante. Pensemos en un conjunto de datos observados de entrenamiento $x$. Puede 
que no conozcamos su distribuci√≥n $p(x)$ pero, ¬øpodemos aprender una distribuci√≥n condicional $p(z|x)$ 
con la cual podamos mapear los datos $x$ a una distribuci√≥n conocida (¬øp. ej. gaussiana? Aunque realmente lo 
podriamos mapear a cualquier distribuci√≥n) ?

Lo que nos gustar√≠a ser√≠a que nuestro encoder calculase la posterior real $p(z|x)$. Pero ya hemos 
visto que para poder hacerlo necesitamos la evidence $p(x)$, que no podemos obtener porque es intratable. ¬øSo what?

Pues si no podemos calcular la posterior exacta $p(z|x)$, la aproximamos. Usamos la distribuci√≥n $q_{\phi}(z|x)$ que calcula
nuestro encoder para que **se parezca lo m√°ximo posible** a la posterior real $p(z|x)$ que no podemos calcular. Y de eso va la inferencia variacional:

::: {.callout-important appearance="minimal"}
**La inferencia variacional (VI) es un m√©todo para estimar una distribuci√≥n compleja (a menudo intratable) 
a partir de una distribuci√≥n conocida**.
:::



Distintos modelos generativos usan trucos distintos para esquivar el problema de la intractabilidad. No podemos 
estimar $p(x)$ directamente, pero ¬øpodemos hacerlo de una forma ligeramente distinta? Y la forma en la que 
el autoencoder variacional lo hace es usando algo conocido como la Evidence Lower Bound o ELBO.


## Evidence Lower Bound

Pff esta siendo denso eh üòÖ. 

Venga, recap. Ahora tenemos dos cosas:

* $q_{\phi}(z|x)$ la distribuci√≥n que aprende nuestro encoder.
* $p(z|x)$: nuestra distribuci√≥n objetivo en la cual estamos interesados, la posterior real.

Nuestro objetivo es minimizar la distancia entre estas dos distribuciones. ¬øY c√≥mo medimos esto? Pues con la **Divergencia KL**:

$$D_{KL}(q_{\phi}(z|x)||p(z|x)) = \sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(z|x)}\right)$$

Es decir, nuestro objetivo va a ser minimizar esta expresi√≥n. 

Vamos a operar un poco. La posterior la podemos sustituir en funcion de la conjunta. Como $p(x,z) = p(z|x) \cdot p(x) \rightarrow p(z|x) = \frac{p(x,z)}{p(x)}$:

$$\sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)p(x)}{p(z,x)}\right)$$

Como $\log(a \cdot b) = \log(a) + \log(b)$, expandimos:

$$\sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(z,x)}\right)+ \sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log(p(x))$$

Ahora podemos hacer dos cosas:

1. El sumatorio del segundo t√©rmino es sobre $z$, por lo que podemos factorizar $\log(p(x))$.
2. $q_{\phi}(z|x)$ es una distribuci√≥n de probabilidad sobre $z$, por lo que 
$\sum_{z \in Z} q_{\phi}(z|x) = 1$ por la condici√≥n de normalizaci√≥n^[Una distribuci√≥n 
de probabilidad v√°lida debe sumar 1 sobre todos sus posibles valores, $\sum_x p(x) = 1$].

Lo que nos dejar√≠a con la expresi√≥n:

$$\sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(x,z)}\right) + \log(p(x))$$

Por la [Desigualdad de Gibbs](#gibbs), sabemos que toda esta expresi√≥n tiene que ser $\geq 0$

$$\sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(x,z)}\right) + \log(p(x)) \geq 0$$

O lo que es lo mismo:

$$ - \sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(x,z)}\right) \leq \log(p(x))$$

De nuevo, por la probabilidad conjunta $p(x,z) = p(x|z)p(z)$:

$$ - \sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(x|z)p(z)}\right) \leq \log(p(x))$$

Y el damos la vuelta al logaritmo para eliminar el negativo:

$$
\underbrace{\sum_{z \in \mathrm{Z}} q_{\phi}(z\mid x)\,
\log\left(\frac{p(x\mid z)\,p(z)}{q_{\phi}(z\mid x)}\right)}_{\text{ELBO}}
\;\le\;
\log p(x)
$$

Este primer t√©rmino es al que se conoce como Evidence Lower Bound o ELBO. 

Hemos llegado a una expresi√≥n en la cual todo es computable o entrenable por una red neuronal. El t√©rmino de la derecha 
sigue siendo intratable, pero **hemos encontrado un l√≠mite inferior**. Hemos encontrado una funci√≥n, una f√≥rmula, 
que ¬°siempre ser√° menor o igual al logar√≠tmo de lo que queremos calcular!

Por tanto, siempre que maximicemos el ELBO, estaremos maximizando tambi√©n $p(x)$ de manera efectiva.

:::{.callout-important appearance="neutral"}
# Main takeaway
Como no podemos obtener $p(x)$ directamente, hemos dado con una expresi√≥n que **da una cota inferior** a $\log p(x)$.  
Mientras **maximicemos** esa cota (la ELBO), estaremos empujando hacia arriba $\log p(x)$ y, en la pr√°ctica, 
resolviendo el problema **sin tener que calcular $p(x)$** expl√≠citamente.
:::


## Reescribiendo ELBO

Ahora nuestro autoencoder se convierte en un problema de m√°ximizaci√≥n del ELBO a partir de los par√°metros del 
encoder $\phi$ y los del decoder $\theta$. Reescrib√°mos la expresi√≥n del ELBO de una forma que tenga 
m√°s sentido para nuestra red neuronal:


$$\sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{p_{\theta}(x|z)\,p(z)}{q_{\phi}(z|x)}\right)
\;\le\;
\log p(x)
$$

La cual podemos expandir usando la regla del producto del logaritmo:

$$\sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log\left(\frac{p(z)}{q_{\phi}(z|x)}\right) 
+ \sum_{z \in \mathrm{Z}} q_{\phi}(z|x) \log(p_{\theta}(x|z))
\;\le\;
\log p(x)
$$

En este punto tenemos dos cosas:

1. El primer t√©rmino tiene la forma de una $D_{KL}$^[Recordemos que $D_{KL}=\sum_{z} 
q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(z)}\right)$], pero con la fracci√≥n invertida: en lugar de tener en el 
numerador la distribuci√≥n que tenemos fuera del logaritmo en la expresi√≥n original sobre 
la cual hacemos el sumatorio, la tenemos en el denominador, por lo que tenemos que darle la vuelta y decir que 
tenemos $- D_{KL} = - \sum_{z} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(z)}\right)$ 

2. El segundo t√©rmino lo que tenemos es el sumatorio de una distribuci√≥n multiplicado por un valor. Eso es la 
esperanza: $\mathbb{E}[X] = \sum_x x_i P(x_i)$. Haciendo la correspondencia $P(x) \leftrightarrow q_{\phi}(z|x)$ y $X(x)  
\leftrightarrow \log(p_{\theta}(x|z))$, podemos reescribirlo en funci√≥n de la esperanza: $\mathbb{E}_{q_{\phi}}[\log(p_{\theta}(x|z))]$

Con lo que nos quedar√≠a:


$$-D_{KL}(q_{\phi}(z|x) || p(z)) + \underbrace{\mathbb{E}_{q_{\phi}}[\log(p_{\theta}(x|z))]}_{\text{Log-likelihood}} \leq \log(p(x))$$


Este segundo t√©rmino se empieza a parecer (***es***) el ***Log-likelihood*** loss. Basicamente, el error de reconstrucci√≥n.


:::{.callout-note}
En la pr√°ctica, el t√©rmino de reconstrucci√≥n no se calcula directamente como log-likelihood, 
sino como MSE. Veremos en la siguiente secci√≥n por qu√© son equivalentes bajo ciertas asunciones.
:::


Con esto, llegamos a la forma final de nuestra funci√≥n de p√©rdida:

$$-D_{KL} + \text{Reconstruction loss} \leq \log(p(x))$${#eq-kl-def}


## Caso gaussiano

Podr√≠amos haber terminado aqu√≠, pero queremos encontrar la forma final de la f√≥rmula. Lo 
que vamos a hacer ahora es computar el $D_{KL}$ espec√≠ficamente para el caso gaussiano (si 
quisi√©semos hacer esto para un caso que no fuese el gaussiano tendriamos que rehacer este c√°lculo).

Partiendo de la ecuaci√≥n:

$$D_{KL}(q_{\phi}(z|x) || p(z)) = \sum_{z} q_{\phi}(z|x) \log\left(\frac{q_{\phi}(z|x)}{p(z)}\right)$$

Y sustituyendo las distribuciones $q_{\phi}(z|x)$ y $p(z)$ por sus respectivas f√≥rmulas anal√≠ticas 
gaussianas, llegamos a la siguiente expresion:

$$D_{KL} = \frac{1}{2}\left(\sigma_q^2 + \mu_q^2 - 1 - \log(\sigma_q^2)\right)$$


:::{.callout-note collapse="true"
title="Paso a paso: $D_{KL}(\mathcal{N}(\mu_q,\sigma_q^2)||\mathcal{N}(0,1))$"
appearance="minimal"}
Nuestras distribuciones son:

* $q_{\phi}(z|x) \sim N(\mu_q, \sigma_q)$: Nuestra distribuci√≥n posterior predicha por el encoder.
* $p(z) \sim N(\mu_p, \sigma_p)$: La distribuci√≥n gaussiana est√°ndar a la cual queremos que se ajuste el espacio latente $z$.

Las gaussianas univariantes de nuestras distribuciones son:

$$q_{\phi}(z|x) = \frac{1}{\sqrt{2\pi\sigma_q^2}} e^{-\frac{(z-\mu_q)^2}{2\sigma_q^2}}$$

$$p(z) = \frac{1}{\sqrt{2\pi\sigma_p^2}} e^{-\frac{(z-\mu_p)^2}{2\sigma_p^2}}$$

Las sustitu√≠mos por lo que tenemos dentro del logar√≠tmo:

$$D_{KL} = \sum_{z}  q_{\phi}p(z|x)\log\left(\frac{\frac{1}{\sqrt{2\pi\sigma_q^2}} e^{-\frac{(z-\mu_q)^2}{2\sigma_q^2}}}{\frac{1}{\sqrt{2\pi\sigma_p^2}} e^{-\frac{(z-\mu_p)^2}{2\sigma_p^2}}}\right)$$

Primero simplificamos el logaritmo. Separamos la fracci√≥n:

$$\log\left(\frac{\frac{1}{\sqrt{2\pi\sigma_q^2}} e^{-\frac{(z-\mu_q)^2}{2\sigma_q^2}}}{\frac{1}{\sqrt{2\pi\sigma_p^2}} e^{-\frac{(z-\mu_p)^2}{2\sigma_p^2}}}\right)$$

Esto es lo mismo que:

$$\log\left(\frac{\sqrt{2\pi\sigma_p^2}}{\sqrt{2\pi\sigma_q^2}} \cdot \frac{e^{-\frac{(z-\mu_q)^2}{2\sigma_q^2}}}{e^{-\frac{(z-\mu_p)^2}{2\sigma_p^2}}}\right)$$

Simplificando los $\sqrt{2\pi}$ y usando $\log(ab) = \log(a) + \log(b)$:

$$\log\left(\frac{\sigma_p}{\sigma_q}\right) + \log\left(e^{-\frac{(z-\mu_q)^2}{2\sigma_q^2} + \frac{(z-\mu_p)^2}{2\sigma_p^2}}\right)$$

Como $\log(e^x) = x$:

$$\log\left(\frac{\sigma_p}{\sigma_q}\right) - \frac{(z-\mu_q)^2}{2\sigma_q^2} + \frac{(z-\mu_p)^2}{2\sigma_p^2}$$

Ahora sustituimos esto en la expresi√≥n completa del $D_{KL}$:

$$D_{KL} = \sum_{z} q_{\phi}(z|x) \left[\log\left(\frac{\sigma_p}{\sigma_q}\right) - \frac{(z-\mu_q)^2}{2\sigma_q^2} + \frac{(z-\mu_p)^2}{2\sigma_p^2}\right]$$

Distribuimos el sumatorio:

$$D_{KL} = \sum_{z} q_{\phi}(z|x) \log\left(\frac{\sigma_p}{\sigma_q}\right) - \sum_{z} q_{\phi}(z|x)\frac{(z-\mu_q)^2}{2\sigma_q^2} + \sum_{z} q_{\phi}(z|x)\frac{(z-\mu_p)^2}{2\sigma_p^2}$$

Ahora simplificamos cada t√©rmino:

1. **Primer t√©rmino:** $\log(\sigma_p/\sigma_q)$ no depende de $z$, por lo que sale fuera del sumatorio. 
Por la condici√≥n de normalizaci√≥n, $\sum_z q_\phi(z|x) = 1$:

$$\sum_{z} q_{\phi}(z|x) \log\left(\frac{\sigma_p}{\sigma_q}\right) = \log\left(\frac{\sigma_p}{\sigma_q}\right)$$

2. **Segundo t√©rmino:** Es la esperanza de $(z-\mu_q)^2$ bajo $q_\phi$, que es la definici√≥n de varianza:

$$\sum_{z} q_{\phi}(z|x)\frac{(z-\mu_q)^2}{2\sigma_q^2} = \frac{1}{2\sigma_q^2} \mathbb{E}_{q_\phi}[(z-\mu_q)^2] = \frac{\sigma_q^2}{2\sigma_q^2} = \frac{1}{2}$$

3. **Tercer t√©rmino:** Usando la identidad $\mathbb{E}[(z-\mu_p)^2] = \text{Var}(z) + (\mathbb{E}[z] - \mu_p)^2 = \sigma_q^2 + (\mu_q - \mu_p)^2$:

$$\sum_{z} q_{\phi}(z|x)\frac{(z-\mu_p)^2}{2\sigma_p^2} = \frac{\sigma_q^2 + (\mu_q - \mu_p)^2}{2\sigma_p^2}$$

Juntando todo:

$$D_{KL} = \log\left(\frac{\sigma_p}{\sigma_q}\right) - \frac{1}{2} + \frac{\sigma_q^2 + (\mu_q - \mu_p)^2}{2\sigma_p^2}$$

Ahora sustituimos los par√°metros del prior $p(z) = \mathcal{N}(0, 1)$, es decir $\mu_p = 0$ y $\sigma_p = 1$:

$$D_{KL} = \log\left(\frac{1}{\sigma_q}\right) - \frac{1}{2} + \frac{\sigma_q^2 + \mu_q^2}{2}$$

Usando $\log(1/x) = -\log(x)$:

$$D_{KL} = -\log(\sigma_q) - \frac{1}{2} + \frac{\sigma_q^2 + \mu_q^2}{2}$$

Reordenando:

$$D_{KL} = -\log(\sigma_q) + \frac{\sigma_q^2 + \mu_q^2}{2} - \frac{1}{2}$$

O equivalentemente:

$$D_{KL} = \frac{1}{2}\left(\sigma_q^2 + \mu_q^2 - 1 - \log(\sigma_q^2)\right)$$

Donde $\mu_q$ y $\sigma_q$ son los par√°metros que produce nuestro encoder para cada input $x$.
:::

Sustituimos en @eq-kl-def:

$$ - \frac{1}{2}\left(\sigma_q^2 + \mu_q^2 - 1 - \log(\sigma_q^2)\right) + \mathbb{E}_{q_{\phi}}[\log(p_{\theta}(x|z))] \leq \log(p(x))$$

Esta es casi nuestra funci√≥n de p√©rdida final. Esto es el ELBO para el caso espec√≠fico gaussiano. Y nuestro objetivo es maximizarlo. 
Nuestro objetivo es, mas concretamente, maximizarlo respecto a los par√°metros $\phi$ y $\theta$ de nuestra red neuronal.

$$\max_{\phi,\theta}  - \frac{1}{2}\left(\sigma_q^2 + \mu_q^2 - 1 - \log(\sigma_q^2)\right) + \mathbb{E}_{q_{\phi}}[\log(p_{\theta}(x|z))]$$

Pero, como todos sabemos, el descenso del gradiente no maximiza, minimiza. Por lo que, lo que haremos ser√° minimizar el negativo
de esta funci√≥n:

$$ \min_{\phi,\theta}  -\left[ - \frac{1}{2}\left(\sigma_q^2 + \mu_q^2 - 1 - \log(\sigma_q^2)\right) + \mathbb{E}_{q_{\phi}}[\log(p_{\theta}(x|z))] \right]$$

Reformulando:

$$ \min_{\phi,\theta}  \frac{1}{2}\left(\sigma_q^2 + \mu_q^2 - 1 - \log(\sigma_q^2)\right) - \mathbb{E}_{q_{\phi}}[\log(p_{\theta}(x|z))]$$

**√âsta es la funci√≥n de p√©rdida que vamos a implementar.**


## Referencias

Una pregunta r√°pida a chatgpt sobre recursos para aprender inferencia variacional nos dar√° este tipo de cosas:

- Kevin Murphy - "Probabilistic Machine Learning", Vol√∫menes 1 y 2. El primero sobre probabilidad, Bayes y modelos gr√°ficos, y el segundo s√≠ que trata m√°s en profundidad el VI.
- David Blei, Alp kucukelbir, Jon McAuliffe (2017) - "Variational Inference: A Review for Statisticians"
- Kingma & Welling (2013) - Auto-Encoding Variational Bayes.
- El m√≠tico "Pattern Recognition and Machine Learning" de Bishop.
- McElreath - Statistical Rethinking.

Tan √∫tiles e interesantes como infumables. 

Yo he seguido este video, que es bastante m√°s claro:

- [Mathing the Variational AutoEncoder: Deriving the ELBO Loss](https://www.youtube.com/watch?v=jJZadDULoH4). Aunque en el desarrollo se le va un poco de las manos, sigue siendo el recurso m√°s claro que he encontrado.


